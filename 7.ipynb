{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020451134_hw7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4koQ3Hy1Jtc",
        "colab_type": "text"
      },
      "source": [
        "#비교결과\n",
        "- CNN : accuracy=0.855\n",
        "- RNN : accuracy=0.801\n",
        "- GRU : accuracy=0.806\n",
        "- LSTM : accuracy=0.826\n",
        "\n",
        "##accuracy : CNN > LSTM > GRU > RNN\n",
        "##speed : LSTM > GRU = RNN > CNN\n",
        " \n",
        "정확도와 속도적인 면을 봤을때 CNN을 사용하는 것이 좋을 것 같다.\n",
        "각 모델들의 epoch나 일정부분이 다른 것은 오히려 accuracy 가 떨어지는 경우가 있어서... 각각 돌렸을 때 나왔던 최고의 accuracy를 기입함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6ylqWuAk0qm",
        "colab_type": "text"
      },
      "source": [
        "#CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXAtlCWTnVKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "0d131c57-5be2-4497-fe2d-df34b539f6e1"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "train_data[0] # [1,14,22, ... , 19,178,32]\n",
        "train_labels[0] # 1\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict(\n",
        "    [(value,key) for (key, value) in word_index.items()]\n",
        ")\n",
        "decode_review = ' '.join(\n",
        "    [reverse_word_index.get(i-3, '?') for i in train_data[0]]\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "  \n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16,activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs = 20,\n",
        "                    batch_size = 512,\n",
        "                    validation_data=(x_val, y_val))\n",
        "history_dict = history.history\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1,len(loss)+1)\n",
        "\n",
        "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "# plt.title('Training and validation loss')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# history = model.fit(partial_x_train,\n",
        "#                     partial_y_train,\n",
        "#                     epochs = 5,\n",
        "#                     batch_size = 512,\n",
        "#                     validation_data=(x_val, y_val))\n",
        "\n",
        "#results = model.evaluate(x_test, y_test)\n",
        "loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=2)\n",
        "print('Test performance: accuracy={0}, loss={1}'.format(acc, loss))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "15000/15000 [==============================] - 2s 115us/step - loss: 0.5378 - accuracy: 0.7679 - val_loss: 0.4208 - val_accuracy: 0.8482\n",
            "Epoch 2/20\n",
            "15000/15000 [==============================] - 2s 107us/step - loss: 0.3308 - accuracy: 0.9003 - val_loss: 0.3163 - val_accuracy: 0.8896\n",
            "Epoch 3/20\n",
            "15000/15000 [==============================] - 2s 107us/step - loss: 0.2395 - accuracy: 0.9253 - val_loss: 0.3133 - val_accuracy: 0.8736\n",
            "Epoch 4/20\n",
            "15000/15000 [==============================] - 2s 108us/step - loss: 0.1867 - accuracy: 0.9397 - val_loss: 0.3001 - val_accuracy: 0.8841\n",
            "Epoch 5/20\n",
            "15000/15000 [==============================] - 2s 108us/step - loss: 0.1483 - accuracy: 0.9529 - val_loss: 0.2949 - val_accuracy: 0.8824\n",
            "Epoch 6/20\n",
            "15000/15000 [==============================] - 2s 109us/step - loss: 0.1157 - accuracy: 0.9669 - val_loss: 0.3450 - val_accuracy: 0.8704\n",
            "Epoch 7/20\n",
            "15000/15000 [==============================] - 2s 108us/step - loss: 0.0976 - accuracy: 0.9719 - val_loss: 0.3215 - val_accuracy: 0.8799\n",
            "Epoch 8/20\n",
            "15000/15000 [==============================] - 2s 110us/step - loss: 0.0799 - accuracy: 0.9783 - val_loss: 0.3299 - val_accuracy: 0.8813\n",
            "Epoch 9/20\n",
            "15000/15000 [==============================] - 2s 109us/step - loss: 0.0644 - accuracy: 0.9833 - val_loss: 0.3590 - val_accuracy: 0.8775\n",
            "Epoch 10/20\n",
            "15000/15000 [==============================] - 2s 108us/step - loss: 0.0512 - accuracy: 0.9891 - val_loss: 0.4278 - val_accuracy: 0.8692\n",
            "Epoch 11/20\n",
            "15000/15000 [==============================] - 2s 109us/step - loss: 0.0421 - accuracy: 0.9899 - val_loss: 0.4055 - val_accuracy: 0.8734\n",
            "Epoch 12/20\n",
            "15000/15000 [==============================] - 2s 108us/step - loss: 0.0335 - accuracy: 0.9937 - val_loss: 0.4335 - val_accuracy: 0.8720\n",
            "Epoch 13/20\n",
            "15000/15000 [==============================] - 2s 110us/step - loss: 0.0266 - accuracy: 0.9949 - val_loss: 0.4578 - val_accuracy: 0.8736\n",
            "Epoch 14/20\n",
            "15000/15000 [==============================] - 2s 109us/step - loss: 0.0218 - accuracy: 0.9963 - val_loss: 0.4878 - val_accuracy: 0.8716\n",
            "Epoch 15/20\n",
            "15000/15000 [==============================] - 2s 110us/step - loss: 0.0123 - accuracy: 0.9991 - val_loss: 0.5530 - val_accuracy: 0.8651\n",
            "Epoch 16/20\n",
            "15000/15000 [==============================] - 2s 108us/step - loss: 0.0157 - accuracy: 0.9976 - val_loss: 0.5562 - val_accuracy: 0.8711\n",
            "Epoch 17/20\n",
            "15000/15000 [==============================] - 2s 109us/step - loss: 0.0071 - accuracy: 0.9997 - val_loss: 0.5865 - val_accuracy: 0.8694\n",
            "Epoch 18/20\n",
            "15000/15000 [==============================] - 2s 108us/step - loss: 0.0096 - accuracy: 0.9978 - val_loss: 0.6207 - val_accuracy: 0.8703\n",
            "Epoch 19/20\n",
            "15000/15000 [==============================] - 2s 108us/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 0.8829 - val_accuracy: 0.8433\n",
            "Epoch 20/20\n",
            "15000/15000 [==============================] - 2s 110us/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.6822 - val_accuracy: 0.8693\n",
            "Test performance: accuracy=0.8533999919891357, loss=0.7456746561527252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrm3cszQiBlt",
        "colab_type": "text"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxNUZbOPdGYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "d85b1786-06c3-480c-886d-c496be0b736d"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import layers, models, datasets\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "class MyRNNCell(layers.Layer):\n",
        "\tdef __init__(self, units, **kwargs):\n",
        "\t\tself.units = units\n",
        "\t\tself.state_size = units\n",
        "\t\tsuper(MyRNNCell, self).__init__(**kwargs)\n",
        "\n",
        "\tdef build(self, input_shape):\n",
        "\t\tself.input_kernel = self.add_weight( shape=(input_shape[-1], self.units), initializer='uniform', name='input_kernel')\n",
        "\t\tself.hidden_kernel = self.add_weight( shape=(self.units, self.units), initializer='uniform', name='hidden_kernel')\n",
        "\t\tself.output_kernel = self.add_weight( shape=(self.units, self.units), initializer='uniform', name='output_kernel')\n",
        "\t\tself.hidden_bias = self.add_weight( shape=(self.units,), initializer='zeros', name='hidden_bias')\n",
        "\t\tself.output_bias = self.add_weight( shape=(self.units,), initializer='zeros', name='output_bias')\n",
        "\t\tself.built = True\n",
        "\n",
        "\tdef call(self, inputs, states):\n",
        "\t\tprev_hidden = states[0]\n",
        "\n",
        "\t\th = K.dot(inputs, self.input_kernel) + K.dot(prev_hidden, self.hidden_kernel)\n",
        "\t\th = K.tanh(h + self.hidden_bias)\n",
        "\t\toutput = K.dot(h, self.output_kernel) + self.output_bias\n",
        "\t\treturn output, [h]\n",
        "\n",
        "\tdef compute_output_shape(self, input_shape):\n",
        "\t\treturn (input_shape[0], self.units)\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 80\n",
        "batch_size = 100\n",
        "epochs = 4\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.imdb.load_data(num_words=max_features)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "x = layers.Input((maxlen,))\n",
        "h = layers.Embedding(max_features, 128)(x)\n",
        "h = layers.RNN(MyRNNCell(128))(h)\n",
        "h = layers.Activation('relu')(h)\n",
        "h = layers.Dropout(0.25)(h)\n",
        "y = layers.Dense(1, activation='sigmoid')(h)\n",
        "\n",
        "model = models.Model(x, y)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "print('Training stage')\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), verbose=2)\n",
        "# plt.plot(history.history['loss'], 'y', label='train loss')\n",
        "# plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "# plt.legend(loc='upper right')\n",
        "# plt.show()\n",
        "# plt.plot(history.history['acc'], 'y', label='train acc')\n",
        "# plt.plot(history.history['val_acc'], 'r', label='val acc')\n",
        "# plt.legend(loc='upper right') \n",
        "# plt.show()\n",
        "\n",
        "loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=2)\n",
        "print('Test performance: accuracy={0}, loss={1}'.format(acc, loss))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "embedding_12 (Embedding)     (None, 80, 128)           2560000   \n",
            "_________________________________________________________________\n",
            "rnn_10 (RNN)                 (None, 128)               49408     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,609,537\n",
            "Trainable params: 2,609,537\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training stage\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/4\n",
            " - 32s - loss: 0.5182 - accuracy: 0.7212 - val_loss: 0.3924 - val_accuracy: 0.8275\n",
            "Epoch 2/4\n",
            " - 32s - loss: 0.3105 - accuracy: 0.8730 - val_loss: 0.4217 - val_accuracy: 0.8119\n",
            "Epoch 3/4\n",
            " - 32s - loss: 0.2084 - accuracy: 0.9207 - val_loss: 0.4865 - val_accuracy: 0.7833\n",
            "Epoch 4/4\n",
            " - 32s - loss: 0.1678 - accuracy: 0.9368 - val_loss: 0.5940 - val_accuracy: 0.7983\n",
            "Test performance: accuracy=0.7982800006866455, loss=0.5939794690608978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgjUxDJKiFsN",
        "colab_type": "text"
      },
      "source": [
        "#GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LE6RHoIfALW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "60d41702-de90-4289-a939-c30ddaaaae5e"
      },
      "source": [
        "class MyGRUCell(layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    self.units = units\n",
        "    self.state_size = units\n",
        "    super(MyGRUCell, self).__init__(**kwargs)\n",
        "\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    self.input_update_kernel = self.add_weight(\n",
        "        shape=(input_shape[-1], self.units),\n",
        "        initializer='uniform',\n",
        "        name='input_update_kernel')\n",
        "    self.hidden_update_kernel = self.add_weight(\n",
        "        shape=(self.units, self.units),\n",
        "        initializer='uniform',\n",
        "        name='hidden_update_kernel')\n",
        "    self.input_reset_kernel = self.add_weight(\n",
        "        shape=(input_shape[-1], self.units),\n",
        "        initializer='uniform',\n",
        "        name='input_reset_kernel')\n",
        "    self.hidden_reset_kernel = self.add_weight(\n",
        "        shape=(self.units, self.units),\n",
        "        initializer='uniform',\n",
        "        name='hidden_reset_kernel')\n",
        "    self.input_kernel = self.add_weight(\n",
        "        shape=(input_shape[-1], self.units),\n",
        "        initializer='uniform',\n",
        "        name='input_kernel')\n",
        "    self.hidden_kernel = self.add_weight(\n",
        "        shape=(self.units, self.units),\n",
        "        initializer='uniform',\n",
        "        name='hidden_kernel')\n",
        "    \n",
        "    self.built = True\n",
        "\n",
        "    \n",
        "  def call(self, inputs, states):\n",
        "    prev_hidden = states[0]\n",
        "    \n",
        "    z = K.sigmoid(K.dot(inputs, self.input_update_kernel)\n",
        "                  + K.dot(prev_hidden, self.hidden_update_kernel))\n",
        "    r = K.sigmoid(K.dot(inputs, self.input_reset_kernel)\n",
        "                  + K.dot(prev_hidden, self.hidden_reset_kernel))\n",
        "    h_t = K.tanh(K.dot(inputs, self.input_kernel)\n",
        "                 + r * K.dot(prev_hidden, self.hidden_kernel))\n",
        "    h = z * prev_hidden + (1 - z) * h_t\n",
        "    \n",
        "    return h, [h]\n",
        "  \n",
        "  \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], self.units)\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 80\n",
        "batch_size = 100\n",
        "epochs = 4\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.imdb.load_data(num_words=max_features)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "x = layers.Input((maxlen,))\n",
        "h = layers.Embedding(max_features, 128)(x)\n",
        "h = layers.RNN(MyRNNCell(128))(h)\n",
        "h = layers.Activation('relu')(h)\n",
        "h = layers.Dropout(0.25)(h)\n",
        "y = layers.Dense(1, activation='sigmoid')(h)\n",
        "model = models.Model(x, y)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "print('Training stage')\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), verbose=2)\n",
        "# plt.plot(history.history['loss'], 'y', label='train loss')\n",
        "# plt.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "# plt.legend(loc='upper right')\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(history.history['acc'], 'y', label='train acc')\n",
        "# plt.plot(history.history['val_acc'], 'r', label='val acc')\n",
        "# plt.legend(loc='upper right') \n",
        "# plt.show()\n",
        "\n",
        "loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=2)\n",
        "print('Test performance: accuracy={0}, loss={1}'.format(acc, loss))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "embedding_8 (Embedding)      (None, 80, 128)           2560000   \n",
            "_________________________________________________________________\n",
            "rnn_6 (RNN)                  (None, 128)               49408     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,609,537\n",
            "Trainable params: 2,609,537\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training stage\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/4\n",
            " - 39s - loss: 0.5203 - accuracy: 0.7262 - val_loss: 0.4262 - val_accuracy: 0.8094\n",
            "Epoch 2/4\n",
            " - 42s - loss: 0.3165 - accuracy: 0.8719 - val_loss: 0.4373 - val_accuracy: 0.8148\n",
            "Epoch 3/4\n",
            " - 37s - loss: 0.2213 - accuracy: 0.9143 - val_loss: 0.4667 - val_accuracy: 0.8129\n",
            "Epoch 4/4\n",
            " - 40s - loss: 0.1698 - accuracy: 0.9366 - val_loss: 0.5311 - val_accuracy: 0.8067\n",
            "Test performance: accuracy=0.8066800236701965, loss=0.5311337777376175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcyEKM97iMa5",
        "colab_type": "text"
      },
      "source": [
        "#LSTM\n",
        "- learning 시간 가장 김"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpb1El5HghaD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "f5fde212-c908-46ca-bfdc-b791fc009d3c"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers, models, datasets\n",
        "from keras import backend as K\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection, metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        " \n",
        "class Data:\n",
        "    def __init__(self, max_features=20000, maxlen=80):\n",
        "        (x_train, y_train), (x_test, y_test) = datasets.imdb.load_data(\n",
        "            num_words=max_features)\n",
        "        x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "        x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "        self.x_train, self.y_train = x_train, y_train\n",
        "        self.x_test, self.y_test = x_test, y_test\n",
        " \n",
        "class RNN_LSTM(models.Model):\n",
        "    def __init__(self, max_features, maxlen):\n",
        "        x = layers.Input((maxlen,))\n",
        "        h = layers.Embedding(max_features, 128)(x)\n",
        "        h = layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)(h)\n",
        "        y = layers.Dense(1, activation='sigmoid')(h)\n",
        "        super().__init__(x, y)\n",
        " \n",
        "        self.compile(loss='binary_crossentropy',\n",
        "            optimizer='adam', metrics=['accuracy'])\n",
        " \n",
        "class Machine:\n",
        "    def __init__(self, max_features=20000, maxlen=80):\n",
        "        self.data = Data(max_features, maxlen)\n",
        "        self.model = RNN_LSTM(max_features, maxlen)\n",
        " \n",
        "    def run(self, epochs=4, batch_size=32):\n",
        "        data = self.data\n",
        "        model = self.model\n",
        " \n",
        "        print('Training stage')\n",
        "        model.fit(data.x_train, data.y_train,\n",
        "            batch_size=batch_size, epochs=epochs,\n",
        "            validation_data=(data.x_test, data.y_test),\n",
        "            verbose=2)\n",
        " \n",
        "        loss, acc = model.evaluate(data.x_test, data.y_test,\n",
        "            batch_size=batch_size, verbose=2)\n",
        " \n",
        "        print('Test performance: accuracy={0}, loss={1}'.format(acc, loss))\n",
        " \n",
        "def main():\n",
        "    m = Machine()\n",
        "    m.run()\n",
        " \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training stage\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/4\n",
            " - 140s - loss: 0.4543 - accuracy: 0.7878 - val_loss: 0.3932 - val_accuracy: 0.8269\n",
            "Epoch 2/4\n",
            " - 144s - loss: 0.2986 - accuracy: 0.8810 - val_loss: 0.3840 - val_accuracy: 0.8348\n",
            "Epoch 3/4\n",
            " - 141s - loss: 0.2197 - accuracy: 0.9142 - val_loss: 0.4369 - val_accuracy: 0.8324\n",
            "Epoch 4/4\n",
            " - 141s - loss: 0.1534 - accuracy: 0.9431 - val_loss: 0.4903 - val_accuracy: 0.8268\n",
            "Test performance: accuracy=0.8267999887466431, loss=0.49030957569122313\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}