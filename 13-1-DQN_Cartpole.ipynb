{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"13-1-DQN_Cartpole.ipynb","provenance":[{"file_id":"1ePEeuwsrQrAgR4n7eJg1pY0Mv3Mnx8ZC","timestamp":1591895830578},{"file_id":"1udSxK18VlUwiMP_j3tzRMc0jmLkqTxxO","timestamp":1585506103653}],"collapsed_sections":[],"authorship_tag":"ABX9TyOK9Vac2N+wLLQGMruxd/BK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"aKNsEnCblqGV","colab_type":"text"},"source":["**13-1-DQN_Cartpole.ipynb**\n","\n","Original author: Rowel Atienza, \"Advanced Deep Learning with Keras\" book\n","\n","Code 수정 by In-Kwon Lee, 2020\n"]},{"cell_type":"code","metadata":{"id":"f9VHyM8CL8Tp","colab_type":"code","outputId":"ce4bd05b-1102-4fe3-f4df-4c4f2f16c5fd","executionInfo":{"status":"ok","timestamp":1592153397809,"user_tz":-540,"elapsed":25137,"user":{"displayName":"­이인권(전임교원/공과대학 컴퓨터과학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsSt8xefFvEjcqDUzIshC_aE0AioaTHI518D6M=s64","userId":"09898204514220972787"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["# Colab server에 내 google drive를 mount. /content/drive/My Drive 로 mount됨\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DOA32jCCML4Z","colab_type":"code","outputId":"d03ad972-5f06-49f5-92a0-a236be638bc8","executionInfo":{"status":"ok","timestamp":1592153408603,"user_tz":-540,"elapsed":5606,"user":{"displayName":"­이인권(전임교원/공과대학 컴퓨터과학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsSt8xefFvEjcqDUzIshC_aE0AioaTHI518D6M=s64","userId":"09898204514220972787"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Training된 weight 저장을 위해 temp directory를 생성해 둠\n","# 이미 만들어진 경우는 server가 만들 수 없다고 할 것이나, 신경쓸 필요 없음\n","\n","!mkdir '/content/drive/My Drive/Colab Notebooks/temp'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘/content/drive/My Drive/Colab Notebooks/temp’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nz_ricpPDCca","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from collections import deque            # deque는 처음과 끝 부분에서 모두 insert/delete가 가능한 data structure\n","import numpy as np\n","import random\n","#import argparse                         # console에서 실행할 때는 argparse를 쓰는 것이 편함 (Atienza book original source 참조)\n","import gym                               # openAI gym package를 로딩\n","from gym import wrappers, logger         # gym package 중 wrappers와 logger 사용"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxu9ppjrjGjf","colab_type":"code","colab":{}},"source":["# class DQNAgent\n","\n","class DQNAgent:\n","    def __init__(self,\n","                 state_space, \n","                 action_space, \n","                 episodes=500):\n","        \"\"\"DQN Agent on CartPole-v0 environment\n","\n","        Arguments:\n","            state_space (tensor): state space\n","            action_space (tensor): action space\n","            episodes (int): number of episodes to train\n","        \"\"\"\n","        self.action_space = action_space\n","\n","        # experience buffer\n","        self.memory = []\n","\n","        # discount rate\n","        self.gamma = 0.9\n","\n","        # initially 90% exploration, 10% exploitation\n","        self.epsilon = 1.0\n","        # iteratively applying decay til \n","        # 10% exploration/90% exploitation\n","        self.epsilon_min = 0.1\n","        self.epsilon_decay = self.epsilon_min / self.epsilon\n","        self.epsilon_decay = self.epsilon_decay ** \\\n","                             (1. / float(episodes))\n","\n","        # Q Network weights filename\n","        self.weights_file = 'dqn_cartpole.h5'\n","        # Q Network for training\n","        n_inputs = state_space.shape[0]\n","        n_outputs = action_space.n\n","        self.q_model = self.build_model(n_inputs, n_outputs)\n","        self.q_model.compile(loss='mse', optimizer=Adam())\n","        # target Q Network\n","        self.target_q_model = self.build_model(n_inputs, n_outputs)\n","        # copy Q Network params to target Q Network\n","        self.update_weights()\n","\n","        self.replay_counter = 0\n","\n","    \n","    def build_model(self, n_inputs, n_outputs):\n","        \"\"\"Q Network is 256-256-256 MLP\n","\n","        Arguments:\n","            n_inputs (int): input dim\n","            n_outputs (int): output dim\n","\n","        Return:\n","            q_model (Model): DQN\n","        \"\"\"\n","        inputs = Input(shape=(n_inputs, ), name='state')\n","        x = Dense(256, activation='relu')(inputs)\n","        x = Dense(256, activation='relu')(x)\n","        x = Dense(256, activation='relu')(x)\n","        x = Dense(n_outputs,\n","                  activation='linear', \n","                  name='action')(x)\n","        q_model = Model(inputs, x)\n","        q_model.summary()\n","        return q_model\n","\n","\n","    def save_weights(self):\n","        \"\"\"save Q Network params to a file\"\"\"\n","        self.q_model.save_weights(self.weights_file)\n","\n","\n","    def update_weights(self):\n","        \"\"\"copy trained Q Network params to target Q Network\"\"\"\n","        self.target_q_model.set_weights(self.q_model.get_weights())\n","\n","\n","    def act(self, state):\n","        \"\"\"eps-greedy policy\n","        Return:\n","            action (tensor): action to execute\n","        \"\"\"\n","        if np.random.rand() < self.epsilon:\n","            # explore - do random action\n","            return self.action_space.sample()\n","\n","        # exploit\n","        q_values = self.q_model.predict(state)\n","        # select the action with max Q-value\n","        action = np.argmax(q_values[0])\n","        return action\n","\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        \"\"\"store experiences in the replay buffer\n","        Arguments:\n","            state (tensor): env state\n","            action (tensor): agent action\n","            reward (float): reward received after executing\n","                action on state\n","            next_state (tensor): next state\n","        \"\"\"\n","        item = (state, action, reward, next_state, done)\n","        self.memory.append(item)\n","\n","\n","    def get_target_q_value(self, next_state, reward):\n","        \"\"\"compute Q_max\n","           Use of target Q Network solves the \n","            non-stationarity problem\n","        Arguments:\n","            reward (float): reward received after executing\n","                action on state\n","            next_state (tensor): next state\n","        Return:\n","            q_value (float): max Q-value computed\n","        \"\"\"\n","        # max Q value among next state's actions\n","        # DQN chooses the max Q value among next actions\n","        # selection and evaluation of action is \n","        # on the target Q Network\n","        # Q_max = max_a' Q_target(s', a')\n","        q_value = np.amax(\\\n","                     self.target_q_model.predict(next_state)[0])\n","\n","        # Q_max = reward + gamma * Q_max\n","        q_value *= self.gamma\n","        q_value += reward\n","        return q_value\n","\n","\n","    def replay(self, batch_size):\n","        \"\"\"experience replay addresses the correlation issue \n","            between samples\n","        Arguments:\n","            batch_size (int): replay buffer batch \n","                sample size\n","        \"\"\"\n","        # sars = state, action, reward, state' (next_state)\n","        sars_batch = random.sample(self.memory, batch_size)\n","        state_batch, q_values_batch = [], []\n","\n","        # fixme: for speedup, this could be done on the tensor level\n","        # but easier to understand using a loop\n","        for state, action, reward, next_state, done in sars_batch:\n","            # policy prediction for a given state\n","            q_values = self.q_model.predict(state)\n","            \n","            # get Q_max\n","            q_value = self.get_target_q_value(next_state, reward)\n","\n","            # correction on the Q value for the action used\n","            q_values[0][action] = reward if done else q_value\n","\n","            # collect batch state-q_value mapping\n","            state_batch.append(state[0])\n","            q_values_batch.append(q_values[0])\n","\n","        # train the Q-network\n","        self.q_model.fit(np.array(state_batch),\n","                         np.array(q_values_batch),\n","                         batch_size=batch_size,\n","                         epochs=1,\n","                         verbose=0)\n","\n","        # update exploration-exploitation probability\n","        self.update_epsilon()\n","\n","        # copy new params on old target after \n","        # every 10 training updates\n","        if self.replay_counter % 10 == 0:\n","            self.update_weights()\n","\n","        self.replay_counter += 1\n","\n","    \n","    def update_epsilon(self):\n","        \"\"\"decrease the exploration, increase exploitation\"\"\"\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfqJO5WXZmKU","colab_type":"code","colab":{}},"source":["# class DDQNAgent\n","\n","class DDQNAgent(DQNAgent):\n","    def __init__(self,\n","                 state_space, \n","                 action_space, \n","                 episodes=500):\n","        super().__init__(state_space, \n","                         action_space, \n","                         episodes)\n","        \"\"\"DDQN Agent on CartPole-v0 environment\n","\n","        Arguments:\n","            state_space (tensor): state space\n","            action_space (tensor): action space\n","            episodes (int): number of episodes to train\n","        \"\"\"\n","\n","        # Q Network weights filename\n","        self.weights_file = 'ddqn_cartpole.h5'\n","        print(\"-------------DDQN------------\")\n","\n","    def get_target_q_value(self, next_state, reward):\n","        \"\"\"compute Q_max\n","           Use of target Q Network solves the \n","            non-stationarity problem\n","        Arguments:\n","            reward (float): reward received after executing\n","                action on state\n","            next_state (tensor): next state\n","        Returns:\n","            q_value (float): max Q-value computed\n","        \"\"\"\n","        # max Q value among next state's actions\n","        # DDQN\n","        # current Q Network selects the action\n","        # a'_max = argmax_a' Q(s', a')\n","        action = np.argmax(self.q_model.predict(next_state)[0])\n","        # target Q Network evaluates the action\n","        # Q_max = Q_target(s', a'_max)\n","        q_value = self.target_q_model.predict(\\\n","                                      next_state)[0][action]\n","\n","        # Q_max = reward + gamma * Q_max\n","        q_value *= self.gamma\n","        q_value += reward\n","        return q_value"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cPfX8P5K8Mp","colab_type":"code","colab":{}},"source":["# Main Program Begin Here"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BfCJUzxFLFJ2","colab_type":"code","outputId":"7503a2f8-d7c7-4b3e-81d6-a3ca8909c74c","executionInfo":{"status":"ok","timestamp":1592153425437,"user_tz":-540,"elapsed":4098,"user":{"displayName":"­이인권(전임교원/공과대학 컴퓨터과학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsSt8xefFvEjcqDUzIshC_aE0AioaTHI518D6M=s64","userId":"09898204514220972787"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Training 된 Q-table을 저장할 output path \n","outdir = \"/content/drive/My Drive/Colab Notebooks/temp/dqn-CartPole-v0\"\n","\n","# output path가 제대로 준비되었는지 server ls 명령으로 확인\n","!ls \"/content/drive/My Drive/Colab Notebooks/temp\""],"execution_count":5,"outputs":[{"output_type":"stream","text":["dqn-CartPole-v0  q-learning-FrozenLake-v0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7NKq6O8iNneH","colab_type":"code","outputId":"76d72735-6664-4f29-8e4f-11900b949eb3","executionInfo":{"status":"ok","timestamp":1592153510049,"user_tz":-540,"elapsed":762,"user":{"displayName":"­이인권(전임교원/공과대학 컴퓨터과학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsSt8xefFvEjcqDUzIshC_aE0AioaTHI518D6M=s64","userId":"09898204514220972787"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["#----------------------------------------------------------------------- \n","# 실행 옵션은 여기서 수정\n","#-----------------------------------------------------------------------\n","opt_ddqn = False          # True로 하면 DDQN (Double DQN) mode\n","opt_norender = False      # True로 하면 video가 녹화되지 않음\n","opt_id = 'CartPole-v0'\n","\n","# the number of trials without falling over\n","win_trials = 100\n","\n","# the CartPole-v0 is considered solved if for 100 consecutive trials,\n","# the cart pole has not fallen over and it has achieved an average\n","# reward of 195.0\n","# a reward of +1 is provided for every timestep the pole remains\n","# upright\n","win_reward = { 'CartPole-v0' : 195.0 }\n","\n","# stores the reward per episode\n","scores = deque(maxlen=win_trials)\n","\n","# openAI gym의 print option을 ERROR level로 함. \n","# DEBUG, INFO, WARN, ERROR, DISABLED level이 있음. \n","# https://github.com/openai/gym/blob/master/gym/logger.py\n","logger.setLevel(logger.ERROR)\n","\n","# instantiate a gym environment (CartPole-v0)\n","# gym이 제공하는 'CartPole-v0' environment를 하나 생성\n","env = gym.make('CartPole-v0')\n","\n","# simple environment를 wrapper environment로 확장: N개의 이전 observation등을 buffering하여 이용하는 등의 확장 기능 제공\n","# Monitor: data save할 때 필요\n","#if opt_norender:\n","#    env = wrappers.Monitor(env, directory=outdir, video_callable=False, force=True)\n","#else:\n","#    env = wrappers.Monitor(env, directory=outdir, force=True)\n","\n","env.seed(0)\n","print(env.observation_space.shape)\n","print(env.action_space)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["(4,)\n","Discrete(2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-x-zgCPVN17h","colab_type":"code","outputId":"6d8ddf03-0e00-45f1-a257-e81010d77851","executionInfo":{"status":"ok","timestamp":1591949578861,"user_tz":-540,"elapsed":1000,"user":{"displayName":"­이인권(전임교원/공과대학 컴퓨터과학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsSt8xefFvEjcqDUzIshC_aE0AioaTHI518D6M=s64","userId":"09898204514220972787"}},"colab":{"base_uri":"https://localhost:8080/","height":639}},"source":["# instantiate the DQN/DDQN agent\n","if opt_ddqn:\n","    agent = DDQNAgent(env.observation_space, env.action_space)\n","else:\n","    agent = DQNAgent(env.observation_space, env.action_space)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","state (InputLayer)           [(None, 4)]               0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 256)               1280      \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 256)               65792     \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 256)               65792     \n","_________________________________________________________________\n","action (Dense)               (None, 2)                 514       \n","=================================================================\n","Total params: 133,378\n","Trainable params: 133,378\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"model_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","state (InputLayer)           [(None, 4)]               0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 256)               1280      \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 256)               65792     \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 256)               65792     \n","_________________________________________________________________\n","action (Dense)               (None, 2)                 514       \n","=================================================================\n","Total params: 133,378\n","Trainable params: 133,378\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w8oJ916Wb-88","colab_type":"code","colab":{}},"source":["# should be solved in this number of episodes\n","episode_count = 3000\n","state_size = env.observation_space.shape[0]\n","batch_size = 64"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ubz4EA5ARfGb","colab_type":"code","outputId":"05f47fce-78f2-4165-81ae-a78775740594","executionInfo":{"status":"ok","timestamp":1591969653137,"user_tz":-540,"elapsed":20070525,"user":{"displayName":"­이인권(전임교원/공과대학 컴퓨터과학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgsSt8xefFvEjcqDUzIshC_aE0AioaTHI518D6M=s64","userId":"09898204514220972787"}},"colab":{"base_uri":"https://localhost:8080/","height":484}},"source":["# Q-Learning sampling and fitting\n","for episode in range(episode_count):\n","    state = env.reset()\n","    state = np.reshape(state, [1, state_size])\n","    done = False\n","    total_reward = 0\n","    while not done:\n","        # in CartPole-v0, action=0 is left and action=1 is right\n","        action = agent.act(state)\n","        next_state, reward, done, _ = env.step(action)\n","        # in CartPole-v0:\n","        # state = [pos, vel, theta, angular speed]\n","        next_state = np.reshape(next_state, [1, state_size])\n","        # store every experience unit in replay buffer\n","        agent.remember(state, action, reward, next_state, done)\n","        state = next_state\n","        total_reward += reward\n","\n","\n","    # call experience relay\n","    if len(agent.memory) >= batch_size:\n","        agent.replay(batch_size)\n","\n","    scores.append(total_reward)\n","    mean_score = np.mean(scores)\n","    if mean_score >= win_reward[opt_id] \\\n","            and episode >= win_trials:\n","        print(\"Solved in episode %d: \\\n","               Mean survival = %0.2lf in %d episodes\"\n","              % (episode, mean_score, win_trials))\n","        print(\"Epsilon: \", agent.epsilon)\n","        agent.save_weights()\n","        break\n","    if (episode + 1) % win_trials == 0:\n","        print(\"Episode %d: Mean survival = \\\n","               %0.2lf in %d episodes\" %\n","              ((episode + 1), mean_score, win_trials))\n","\n","# close the env and write monitor result info to disk\n","env.close()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Episode 100: Mean survival =                18.17 in 100 episodes\n","Episode 200: Mean survival =                43.28 in 100 episodes\n","Episode 300: Mean survival =                120.49 in 100 episodes\n","Episode 400: Mean survival =                177.19 in 100 episodes\n","Episode 500: Mean survival =                155.40 in 100 episodes\n","Episode 600: Mean survival =                178.83 in 100 episodes\n","Episode 700: Mean survival =                184.98 in 100 episodes\n","Episode 800: Mean survival =                188.66 in 100 episodes\n","Episode 900: Mean survival =                191.02 in 100 episodes\n","Episode 1000: Mean survival =                190.85 in 100 episodes\n","Episode 1100: Mean survival =                188.37 in 100 episodes\n","Episode 1200: Mean survival =                185.59 in 100 episodes\n","Episode 1300: Mean survival =                181.93 in 100 episodes\n","Episode 1400: Mean survival =                184.81 in 100 episodes\n","Episode 1500: Mean survival =                177.02 in 100 episodes\n","Episode 1600: Mean survival =                173.74 in 100 episodes\n","Episode 1700: Mean survival =                189.14 in 100 episodes\n","Episode 1800: Mean survival =                188.80 in 100 episodes\n","Episode 1900: Mean survival =                187.23 in 100 episodes\n","Episode 2000: Mean survival =                188.24 in 100 episodes\n","Episode 2100: Mean survival =                191.13 in 100 episodes\n","Episode 2200: Mean survival =                183.07 in 100 episodes\n","Episode 2300: Mean survival =                188.69 in 100 episodes\n","Episode 2400: Mean survival =                185.41 in 100 episodes\n","Episode 2500: Mean survival =                193.65 in 100 episodes\n","Solved in episode 2503:                Mean survival = 195.33 in 100 episodes\n","Epsilon:  0.09954054173515399\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1g5-GxSwQpD0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}